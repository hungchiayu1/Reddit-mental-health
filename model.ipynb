{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "41e695dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "6ca2ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_df = pd.read_json('data/small_dataset_train.json')\n",
    "large_train_df = pd.read_json('data/large_dataset_train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "eb6d4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_train_embedding = torch.load('data/large_dataset_train_embedding.pt')\n",
    "small_train_embedding = torch.load('data/small_dataset_train_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5e3cb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('data/test.json')\n",
    "test_embedding = torch.load('data/test_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "56786555",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "\n",
    "def pad_tensor_sequence(sequence, max_length, embedding_dim, padding_value =1):\n",
    "    if sequence.size(0)>max_length:\n",
    "        sequence = sequence[:max_length,:] ## Take the first max _length vector\n",
    "    padding = torch.full((max_length - sequence.size(0), embedding_dim), 0)\n",
    "    \n",
    "    \n",
    "    padded_sequence = torch.cat((sequence, padding), dim=0)\n",
    "    \n",
    "    attn_mask = torch.tensor(sequence.size(0)*[0]+padding.size(0)*[padding_value],dtype=torch.float) ##1 if it is a pad token\n",
    "    \n",
    "    return padded_sequence,attn_mask\n",
    "\n",
    "\n",
    "def data_collator_with_padding(batch, embedding_dim, padding_value=0,max_length=128):\n",
    "   \n",
    "    batch_data_attn_mask = [pad_tensor_sequence(item[0], max_length, embedding_dim, padding_value) for item in batch]\n",
    "    batch_labels = [torch.nn.functional.one_hot(torch.tensor(item[1],dtype=torch.long),7) for item in batch]\n",
    "   \n",
    "    batch_data = [item[0] for item in batch_data_attn_mask]\n",
    "    batch_attention_mask = torch.stack([item[1] for item in batch_data_attn_mask])\n",
    "    \n",
    "    batch_data_tensor = torch.stack(batch_data)\n",
    "    #print(batch_labels)\n",
    "    batch_labels_tensor = torch.stack(batch_labels)\n",
    "    \n",
    "    #batch_labels_tensor = torch.tensor(batch_labels)\n",
    "\n",
    "    return batch_data_tensor, batch_labels_tensor.float(), batch_attention_mask\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "\n",
    "#data = [v for v in neg_post_embedding_dict.values()]\n",
    "#data.extend([p for p in adhd_post_embedding_dict.values()])\n",
    "#labels = [0]*len(neg_post_embedding_dict)\n",
    "#labels.extend([1]*len(adhd_post_embedding_dict))\n",
    "\n",
    "\n",
    "#dataloader = DataLoader(dataset, batch_size=16, collate_fn=lambda batch: data_collator_with_padding(batch, 768))\n",
    "\n",
    "#for batch_data, batch_labels,batch_attention_mask in a:\n",
    "    \n",
    "    #pass#print(\"Batch data shape:\", batch_data.shape)\n",
    "   # print(\"Batch labels:\", batch_labels)\n",
    "\n",
    "def create_dataloader_from_post_embedding(post_embedding,df,batch_size):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for author,frame in df.groupby('author'):\n",
    "        data.append(post_embedding[frame.index])\n",
    "        labels.append(frame['label'].iloc[0])\n",
    "        \n",
    "    dataset = CustomDataset(data,labels)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=lambda batch: data_collator_with_padding(batch, 768))\n",
    "    \n",
    "    return dataloader\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "1b0c2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(df,userembedder,embedding_matrix):\n",
    "    author2label = {}\n",
    "    for group,frame in df.groupby('author'):\n",
    "        author2label[group] = frame.label.iloc[0]\n",
    "   \n",
    "    author2embedding = {}\n",
    "    \n",
    "    for user,frame in df.groupby('author'):\n",
    "        \n",
    "        user_post_embedding = embedding_matrix[frame.index] ## This is the current user's post embedding\n",
    "        inp,attn = pad_tensor_sequence(user_post_embedding,128,768)\n",
    "        with torch.no_grad():\n",
    "            inp = inp.unsqueeze(0).to(device)\n",
    "            attn = attn.unsqueeze(0).to(device)\n",
    "            out = userembedder(inp,attn)\n",
    "        \n",
    "            author2embedding[user] = out.squeeze()\n",
    "        \n",
    "        \n",
    "    label2embedding = {i:[] for i in author2label.values()}\n",
    "    \n",
    "    for author,label in author2label.items():\n",
    "        emb = author2embedding[author]\n",
    "        label2embedding[label].append(emb)\n",
    "        \n",
    "    centroids = {key:torch.mean(torch.stack(value),axis=0) for key,value in label2embedding.items()}\n",
    "    return label2embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ba4d26ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(df,userembedder,embedding_matrix,centroid):\n",
    "    centroid_matrix = torch.tensor(torch.stack([v for v in centroids.values()]))\n",
    "    all_labels = []\n",
    "    all_pred = []\n",
    "    for user,frame in df.groupby('author'):\n",
    "        user_post_embedding = embedding_matrix[frame.index] ## This is the current user's post embedding\n",
    "        label = frame.label.iloc[0]\n",
    "        all_labels.append(label)\n",
    "        inp,attn = pad_tensor_sequence(user_post_embedding,128,768)\n",
    "        with torch.no_grad():\n",
    "            inp = inp.unsqueeze(0).to(device)\n",
    "            attn = attn.unsqueeze(0).to(device)\n",
    "            out = userembedder(inp,attn)\n",
    "            \n",
    "            score = torch.cosine_similarity(out,centroid_matrix)\n",
    "            pred = torch.argmax(score)\n",
    "            all_pred.append(pred)\n",
    "            \n",
    "    return torch.tensor(all_pred),torch.tensor(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4d8a201c",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_train_dataloader = create_dataloader_from_post_embedding(large_train_embedding,large_train_df,32)\n",
    "small_train_dataloader =  create_dataloader_from_post_embedding(small_train_embedding,small_train_df,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6d59169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = create_dataloader_from_post_embedding(test_embedding,small_test_df,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68bdcaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class UserEmbedder(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_layer=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.MultiheadAttention(768, 6,batch_first=True) for _ in range(n_layer)])\n",
    "        self.layer_norm = nn.ModuleList([nn.LayerNorm(768) for _ in range(n_layer)])\n",
    "        \n",
    "    def forward(self,x,key_padding_mask=None):\n",
    "        residual = x\n",
    "        for multihead_attention,layer_norm in zip(self.layers,self.layer_norm):\n",
    "           \n",
    "            x,_ = multihead_attention(x,x,x,key_padding_mask=key_padding_mask)\n",
    "            x = residual+x\n",
    "            x = layer_norm(x)\n",
    "            residual = x\n",
    "        x = torch.mean(x,axis=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class Classifier(nn.Module):\n",
    "    \n",
    "    def __init__(self,n_layer=4,userembedder=None):\n",
    "        super().__init__()\n",
    "        if userembedder:\n",
    "            self.userembedder = userembedder\n",
    "        else:\n",
    "            self.userembedder = UserEmbedder(4)\n",
    "        self.fc = nn.Linear(768,64)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc2 = nn.Linear(64,7)\n",
    "        \n",
    "    def forward(self,x,src_mask=None):\n",
    "        x = self.userembedder(x,key_padding_mask=src_mask)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "306f440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from math import log\n",
    "class SupConLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        \"\"\"\n",
    "        Implementation of the loss described in the paper Supervised Contrastive Learning :\n",
    "        https://arxiv.org/abs/2004.11362\n",
    "        :param temperature: int\n",
    "        \"\"\"\n",
    "        super(SupConLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cos = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, projections, targets):\n",
    "        \"\"\"\n",
    "        :param projections: torch.Tensor, shape [batch_size, projection_dim]\n",
    "        :param targets: torch.Tensor, shape [batch_size]\n",
    "        :return: torch.Tensor, scalar\n",
    "        \"\"\"\n",
    "        device = torch.device(\"cuda\") if projections.is_cuda else torch.device(\"cpu\")\n",
    "\n",
    "        # dot_product_tempered = torch.mm(projections, projections.T) / self.temperature\n",
    "        dot_product_tempered = self.cos(projections.unsqueeze(1), projections.unsqueeze(0)) / self.temperature\n",
    "        # Minus max for numerical stability with exponential. Same done in cross entropy. Epsilon added to avoid log(0)\n",
    "        exp_dot_tempered = (\n",
    "            torch.exp(dot_product_tempered - torch.max(dot_product_tempered, dim=1, keepdim=True)[0]) + 1e-5\n",
    "        )\n",
    "\n",
    "        mask_similar_class = (targets.unsqueeze(1).repeat(1, targets.shape[0]) == targets).to(device)\n",
    "        mask_anchor_out = (1 - torch.eye(exp_dot_tempered.shape[0])).to(device)\n",
    "        mask_combined = mask_similar_class * mask_anchor_out\n",
    "        cardinality_per_samples = torch.sum(mask_combined, dim=1)\n",
    "        ## to avoid nan value of the loss if there is only one sample of a category on the batch\n",
    "        for i in range(cardinality_per_samples.size(0)):\n",
    "            if cardinality_per_samples[i]==0:\n",
    "                cardinality_per_samples[i] = 1\n",
    "\n",
    "        log_prob = -torch.log(exp_dot_tempered / (torch.sum(exp_dot_tempered * mask_anchor_out, dim=1, keepdim=True)))\n",
    "        supervised_contrastive_loss_per_sample = torch.sum(log_prob * mask_combined, dim=1) / cardinality_per_samples\n",
    "        supervised_contrastive_loss = torch.mean(supervised_contrastive_loss_per_sample)\n",
    "\n",
    "        return supervised_contrastive_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5ac806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_dataloader,device):\n",
    "    \n",
    "    print(\"----- Evaluating ------\")\n",
    "    \n",
    "    model.eval()\n",
    "   \n",
    "    all_predictions = torch.tensor([])\n",
    "    all_labels = torch.tensor([])\n",
    "   \n",
    "    model = model.to(device)\n",
    "    \n",
    "    with torch.no_grad(): ## Disable gradient\n",
    "         for inputs, labels,attn_mask in tqdm(val_dataloader):\n",
    "\n",
    "            #inputs,labels = batch\n",
    "            #print(inputs,attn_mask)\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            attn_mask = attn_mask.to(device)\n",
    "            \n",
    "            logits = model(inputs,attn_mask)\n",
    "            \n",
    "            \n",
    "            \n",
    "            max_index = torch.argmax(logits,axis=-1).cpu()\n",
    "            pred = torch.nn.functional.one_hot(max_index,7)\n",
    "            \n",
    "            labels = labels.cpu()\n",
    "            all_predictions = torch.cat((all_predictions,pred),axis=0)\n",
    "            all_labels = torch.cat((all_labels,labels),axis=0)\n",
    "\n",
    "    class_indices = torch.argmax(all_labels,axis=1)\n",
    "    prediction_indices = torch.argmax(all_predictions,axis=1)\n",
    "    f1 = f1_score(all_predictions,all_labels,average=None)\n",
    "    acc = torch.sum(class_indices==prediction_indices)/len(prediction_indices)\n",
    "    print(f1)\n",
    "    print(\"ACC\",acc)\n",
    "    return {\"pred\":all_predictions,\"labels\":all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c343bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                 | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Evaluating ------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                                                                                                | 0/31 [00:00<?, ?it/s]\u001b[A/home/cyhung/home/miniconda3/envs/generation_classification/lib/python3.7/site-packages/torch/nn/modules/activation.py:1138: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ../aten/src/ATen/native/transformers/attention.cpp:126.)\n",
      "  1 if key_padding_mask is not None else 0 if attn_mask is not None else None)\n",
      "\n",
      " 16%|███████████████████████████                                                                                                                                             | 5/31 [00:00<00:00, 45.13it/s]\u001b[A\n",
      " 35%|███████████████████████████████████████████████████████████▎                                                                                                           | 11/31 [00:00<00:00, 49.99it/s]\u001b[A\n",
      " 55%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 17/31 [00:00<00:00, 51.66it/s]\u001b[A\n",
      " 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 23/31 [00:00<00:00, 52.25it/s]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 31/31 [00:00<00:00, 53.26it/s]\u001b[A\n",
      " 14%|████████████████████████▏                                                                                                                                                | 1/7 [00:07<00:42,  7.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14285714 0.11049724 0.23003195 0.         0.4159132  0.\n",
      " 0.65700483]\n",
      "ACC tensor(0.3226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|████████████████████████▏                                                                                                                                                | 1/7 [00:09<00:58,  9.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_777508/2805063084.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Classifier(4)\n",
    "#model = Naive()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = 'cuda:1'\n",
    "loss_fn.to(device)\n",
    "for i in tqdm(range(7)):\n",
    "    model.train()\n",
    "    for inp,label,attn in train_dataloader:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inp,label,attn = inp.to(device),label.to(device),attn.to(device)\n",
    "        model = model.to(device)\n",
    "        logits = model(inp,attn)\n",
    "        loss = loss_fn(logits,label)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    evaluate(model,test_dataloader,'cuda:1')\n",
    "    #print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca1683",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model,test_dataloader,\"cuda:1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
